{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building Unet by dividing encoder and decoder into blocks\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, concatenate, Conv2DTranspose, BatchNormalization, Dropout, Lambda\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.layers import Activation, MaxPool2D, Concatenate\n",
    "\n",
    "\n",
    "def conv_block(input, num_filters):\n",
    "    x = Conv2D(num_filters, 3, padding=\"same\")(input)\n",
    "    x = BatchNormalization()(x)   #Not in the original network. \n",
    "    x = Activation(\"relu\")(x)\n",
    "\n",
    "    x = Conv2D(num_filters, 3, padding=\"same\")(x)\n",
    "    x = BatchNormalization()(x)  #Not in the original network\n",
    "    x = Activation(\"relu\")(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "#Encoder block: Conv block followed by maxpooling\n",
    "\n",
    "\n",
    "def encoder_block(input, num_filters):\n",
    "    x = conv_block(input, num_filters)\n",
    "    p = MaxPool2D((2, 2))(x)\n",
    "    return x, p   \n",
    "\n",
    "#Decoder block\n",
    "#skip features gets input from encoder for concatenation\n",
    "\n",
    "def decoder_block(input, skip_features, num_filters):\n",
    "    x = Conv2DTranspose(num_filters, (2, 2), strides=2, padding=\"same\")(input)\n",
    "    x = Concatenate()([x, skip_features])\n",
    "    x = conv_block(x, num_filters)\n",
    "    return x\n",
    "\n",
    "#Build Unet using the blocks\n",
    "def build_unet(input_shape, n_classes):\n",
    "    inputs = Input(input_shape)\n",
    "\n",
    "    s1, p1 = encoder_block(inputs, 64)\n",
    "    s2, p2 = encoder_block(p1, 128)\n",
    "    s3, p3 = encoder_block(p2, 256)\n",
    "    s4, p4 = encoder_block(p3, 512)\n",
    "\n",
    "    b1 = conv_block(p4, 1024) #Bridge\n",
    "\n",
    "    d1 = decoder_block(b1, s4, 512)\n",
    "    d2 = decoder_block(d1, s3, 256)\n",
    "    d3 = decoder_block(d2, s2, 128)\n",
    "    d4 = decoder_block(d3, s1, 64)\n",
    "\n",
    "    if n_classes == 1:  #Binary\n",
    "      activation = 'sigmoid'\n",
    "    else:\n",
    "      activation = 'softmax'\n",
    "\n",
    "    outputs = Conv2D(n_classes, 1, padding=\"same\", activation=activation)(d4)  #Change the activation based on n_classes\n",
    "    print(activation)\n",
    "\n",
    "    model = Model(inputs, outputs, name=\"U-Net\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import cv2\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_img_dir = \"../data/example/train_images/train/\"\n",
    "train_mask_dir = \"../data/example/train_masks/train/\"\n",
    "\n",
    "img_list = os.listdir(train_img_dir)\n",
    "img_list.sort()\n",
    "msk_list = os.listdir(train_mask_dir)\n",
    "msk_list.sort()\n",
    "num_images = len(os.listdir(train_img_dir))\n",
    "print(\"Total number of training images are: \", num_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_num = random.randint(0, num_images-1)\n",
    "\n",
    "img_for_plot = cv2.imread(train_img_dir+img_list[img_num], 0)\n",
    "mask_for_plot =cv2.imread(train_mask_dir+msk_list[img_num], 0)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.subplot(121)\n",
    "plt.imshow(img_for_plot, cmap='gray')\n",
    "plt.title('Image')\n",
    "plt.subplot(122)\n",
    "plt.imshow(mask_for_plot, cmap='gray')\n",
    "plt.title('Mask')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Unique values in the mask are: \", np.unique(mask_for_plot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sample code to encode this single image. We will include this as part of our data gen. \n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "labelencoder = LabelEncoder()\n",
    "h, w = mask_for_plot.shape  \n",
    "mask_for_plot_reshaped = mask_for_plot.reshape(-1,1)\n",
    "mask_for_plot_reshaped_encoded = labelencoder.fit_transform(mask_for_plot_reshaped)\n",
    "mask_for_plot_encoded = mask_for_plot_reshaped_encoded.reshape(h, w)\n",
    "print(\"Unique values in the mask after endcoding are: \", np.unique(mask_for_plot_encoded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=24\n",
    "batch_size= 1\n",
    "n_classes=4\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "#Define a function to perform additional preprocessing after datagen.\n",
    "#For example, scale images, convert masks to categorical, etc. \n",
    "def preprocess_data(img, mask, num_class):\n",
    "    #Scale images\n",
    "    img = img / 255. #This can be done in ImageDataGenerator but showing it outside as an example\n",
    "    #Convert mask to one-hot\n",
    "    labelencoder = LabelEncoder()\n",
    "    n, h, w, c = mask.shape  \n",
    "    mask = mask.reshape(-1,1)\n",
    "    mask = labelencoder.fit_transform(mask)\n",
    "    mask = mask.reshape(n, h, w, c)\n",
    "    mask = to_categorical(mask, num_class)\n",
    "      \n",
    "    return (img, mask)\n",
    "\n",
    "#Define the generator.\n",
    "#We are not doing any rotation or zoom to make sure mask values are not interpolated.\n",
    "#It is important to keep pixel values in mask as 0, 1, 2, 3, .....\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "def trainGenerator(train_img_path, train_mask_path, num_class):\n",
    "    \n",
    "    img_data_gen_args = dict(horizontal_flip=True,\n",
    "                      vertical_flip=True,\n",
    "                      fill_mode='reflect')\n",
    "    \n",
    "    image_datagen = ImageDataGenerator(**img_data_gen_args)\n",
    "    mask_datagen = ImageDataGenerator(**img_data_gen_args)\n",
    "    \n",
    "    image_generator = image_datagen.flow_from_directory(\n",
    "        train_img_path,\n",
    "        class_mode = None,\n",
    "        color_mode = 'grayscale',\n",
    "        target_size=(512,512),\n",
    "        batch_size = batch_size,\n",
    "        seed = seed)\n",
    "    \n",
    "    mask_generator = mask_datagen.flow_from_directory(\n",
    "        train_mask_path,\n",
    "        class_mode = None,\n",
    "        color_mode = 'grayscale',\n",
    "        target_size=(512,512),\n",
    "        batch_size = batch_size,\n",
    "        seed = seed)\n",
    "    \n",
    "    train_generator = zip(image_generator, mask_generator)\n",
    "    \n",
    "    for (img, mask) in train_generator:\n",
    "        img, mask = preprocess_data(img, mask, num_class)\n",
    "        yield (img, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_img_path = \"../data/example/train_images/\"\n",
    "train_mask_path = \"../data/example/train_masks/\"\n",
    "train_img_gen = trainGenerator(train_img_path, train_mask_path, num_class=4)\n",
    "\n",
    "val_img_path = \"../data/example/val_images/\"\n",
    "val_mask_path = \"../data/example/val_masks/\"\n",
    "val_img_gen = trainGenerator(val_img_path, val_mask_path, num_class=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = train_img_gen.__next__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = val_img_gen.__next__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x.shape)\n",
    "print(y.shape)\n",
    "print(\"max value in image dataset is: \", x.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make sure the generator is working and that images and masks are indeed lined up. \n",
    "#Verify generator.... In python 3 next() is renamed as __next__()\n",
    "x, y = train_img_gen.__next__()\n",
    "\n",
    "for i in range(0,1):\n",
    "    image = x[i,:,:,0]\n",
    "    mask = np.argmax(y[i], axis=2)\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.imshow(image, cmap='gray')\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.imshow(mask, cmap='gray')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val, y_val = val_img_gen.__next__()\n",
    "\n",
    "for i in range(0,1):\n",
    "    image = x_val[i,:,:,0]\n",
    "    mask = np.argmax(y_val[i], axis=2)\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.imshow(image, cmap='gray')\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.imshow(mask, cmap='gray')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the model metrcis and load model. \n",
    "\n",
    "num_train_imgs = len(os.listdir('../data/example/train_images/train/'))\n",
    "num_val_images = len(os.listdir('../data/example/val_images/val/'))\n",
    "steps_per_epoch = num_train_imgs//batch_size\n",
    "val_steps_per_epoch = num_val_images//batch_size\n",
    "\n",
    "IMG_HEIGHT = x.shape[1]\n",
    "IMG_WIDTH  = x.shape[2]\n",
    "IMG_CHANNELS = x.shape[3]\n",
    "input_shape = (IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS)\n",
    "n_classes=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_unet(input_shape, n_classes=4)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history=model.fit(train_img_gen,\n",
    "          steps_per_epoch=steps_per_epoch,\n",
    "          epochs=10,\n",
    "          verbose=1,\n",
    "          validation_data=val_img_gen,\n",
    "          validation_steps=val_steps_per_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the model for future use\n",
    "model.save('../models/05_27_onesample_10epochs.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the training and validation accuracy and loss at each epoch\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1, len(loss) + 1)\n",
    "plt.plot(epochs, loss, 'y', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "plt.plot(epochs, acc, 'y', label='Training Accuracy')\n",
    "plt.plot(epochs, val_acc, 'r', label='Validation Accuracy')\n",
    "plt.title('Training and validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load previously saved model\n",
    "from keras.models import load_model\n",
    "model = load_model(\"../models/05_27_onesample_10epochs.hdf5\", compile=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=model.predict(test_image_batch)\n",
    "y_pred_argmax=np.argmax(y_pred, axis=3)\n",
    "y_argmax = np.argmax(test_mask_batch, axis=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using built in keras function\n",
    "from keras.metrics import MeanIoU\n",
    "n_classes = 4\n",
    "IOU_keras = MeanIoU(num_classes=n_classes)  \n",
    "IOU_keras.update_state(y_argmax, y_pred_argmax)\n",
    "print(\"Mean IoU =\", IOU_keras.result().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To calculate I0U for each class...\n",
    "values = np.array(IOU_keras.get_weights()).reshape(n_classes, n_classes)\n",
    "print(values)\n",
    "class1_IoU = values[0,0]/(values[0,0] + values[0,1] + values[0,2] + values[0,3] + values[1,0]+ values[2,0]+ values[3,0])\n",
    "class2_IoU = values[1,1]/(values[1,1] + values[1,0] + values[1,2] + values[1,3] + values[0,1]+ values[2,1]+ values[3,1])\n",
    "class3_IoU = values[2,2]/(values[2,2] + values[2,0] + values[2,1] + values[2,3] + values[0,2]+ values[1,2]+ values[3,2])\n",
    "class4_IoU = values[3,3]/(values[3,3] + values[3,0] + values[3,1] + values[3,2] + values[0,3]+ values[1,3]+ values[2,3])\n",
    "\n",
    "print(\"IoU for class1 is: \", class1_IoU)\n",
    "print(\"IoU for class2 is: \", class2_IoU)\n",
    "print(\"IoU for class3 is: \", class3_IoU)\n",
    "print(\"IoU for class4 is: \", class4_IoU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predict on a few images\n",
    "#model = get_model()\n",
    "#model.load_weights('???.hdf5')  \n",
    "import random\n",
    "test_img_number = random.randint(0, len(test_image_batch)-1)\n",
    "test_img = test_image_batch[test_img_number]\n",
    "ground_truth=test_mask_batch[test_img_number]\n",
    "ground_truth = np.argmax(ground_truth, axis=2)\n",
    "test_img_norm=test_img[:,:,0][:,:,None]\n",
    "test_img_input=np.expand_dims(test_img_norm, 0)\n",
    "prediction = (model.predict(test_img_input))\n",
    "predicted_img=np.argmax(prediction, axis=3)[0,:,:]\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.subplot(231)\n",
    "plt.title('Testing Image')\n",
    "plt.imshow(test_img[:,:,0], cmap='gray')\n",
    "plt.subplot(232)\n",
    "plt.title('Testing Label')\n",
    "plt.imshow(ground_truth, cmap='jet')\n",
    "plt.subplot(233)\n",
    "plt.title('Prediction on test image')\n",
    "plt.imshow(predicted_img, cmap='jet')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fce1c7022bfe12e6241fab6e134680d8fdeb514a4f69a8524f2634ae3ff58bfb"
  },
  "kernelspec": {
   "display_name": "Python 3.9.8 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
