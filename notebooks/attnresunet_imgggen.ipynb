{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from datetime import datetime\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import segmentation_models as sm\n",
    "import tensorflow as tf\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Useful blocks to build Unet\n",
    "conv - BN - Activation - conv - BN - Activation - Dropout (if enabled)\n",
    "'''\n",
    "\n",
    "\n",
    "def conv_block(x, filter_size, size, dropout, batch_norm=False):\n",
    "    \n",
    "    conv = layers.Conv2D(size, (filter_size, filter_size), padding=\"same\")(x)\n",
    "    if batch_norm is True:\n",
    "        conv = layers.BatchNormalization(axis=3)(conv)\n",
    "    conv = layers.Activation(\"relu\")(conv)\n",
    "\n",
    "    conv = layers.Conv2D(size, (filter_size, filter_size), padding=\"same\")(conv)\n",
    "    if batch_norm is True:\n",
    "        conv = layers.BatchNormalization(axis=3)(conv)\n",
    "    conv = layers.Activation(\"relu\")(conv)\n",
    "    \n",
    "    if dropout > 0:\n",
    "        conv = layers.Dropout(dropout)(conv)\n",
    "\n",
    "    return conv\n",
    "\n",
    "\n",
    "def repeat_elem(tensor, rep):\n",
    "    # lambda function to repeat Repeats the elements of a tensor along an axis\n",
    "    #by a factor of rep.\n",
    "    # If tensor has shape (None, 256,256,3), lambda will return a tensor of shape \n",
    "    #(None, 256,256,6), if specified axis=3 and rep=2.\n",
    "\n",
    "     return layers.Lambda(lambda x, repnum: K.repeat_elements(x, repnum, axis=3),\n",
    "                          arguments={'repnum': rep})(tensor)\n",
    "\n",
    "\n",
    "def res_conv_block(x, filter_size, size, dropout, batch_norm=False):\n",
    "    '''\n",
    "    Residual convolutional layer.\n",
    "    Two variants....\n",
    "    Either put activation function before the addition with shortcut\n",
    "    or after the addition (which would be as proposed in the original resNet).\n",
    "    \n",
    "    1. conv - BN - Activation - conv - BN - Activation \n",
    "                                          - shortcut  - BN - shortcut+BN\n",
    "                                          \n",
    "    2. conv - BN - Activation - conv - BN   \n",
    "                                     - shortcut  - BN - shortcut+BN - Activation                                     \n",
    "    \n",
    "    Check fig 4 in https://arxiv.org/ftp/arxiv/papers/1802/1802.06955.pdf\n",
    "    '''\n",
    "\n",
    "    conv = layers.Conv2D(size, (filter_size, filter_size), padding='same')(x)\n",
    "    if batch_norm is True:\n",
    "        conv = layers.BatchNormalization(axis=3)(conv)\n",
    "    conv = layers.Activation('relu')(conv)\n",
    "    \n",
    "    conv = layers.Conv2D(size, (filter_size, filter_size), padding='same')(conv)\n",
    "    if batch_norm is True:\n",
    "        conv = layers.BatchNormalization(axis=3)(conv)\n",
    "    #conv = layers.Activation('relu')(conv)    #Activation before addition with shortcut\n",
    "    if dropout > 0:\n",
    "        conv = layers.Dropout(dropout)(conv)\n",
    "\n",
    "    shortcut = layers.Conv2D(size, kernel_size=(1, 1), padding='same')(x)\n",
    "    if batch_norm is True:\n",
    "        shortcut = layers.BatchNormalization(axis=3)(shortcut)\n",
    "\n",
    "    res_path = layers.add([shortcut, conv])\n",
    "    res_path = layers.Activation('relu')(res_path)    #Activation after addition with shortcut (Original residual block)\n",
    "    return res_path\n",
    "\n",
    "def gating_signal(input, out_size, batch_norm=False):\n",
    "    \"\"\"\n",
    "    resize the down layer feature map into the same dimension as the up layer feature map\n",
    "    using 1x1 conv\n",
    "    :return: the gating feature map with the same dimension of the up layer feature map\n",
    "    \"\"\"\n",
    "    x = layers.Conv2D(out_size, (1, 1), padding='same')(input)\n",
    "    if batch_norm:\n",
    "        x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "    return x\n",
    "\n",
    "def attention_block(x, gating, inter_shape):\n",
    "    shape_x = K.int_shape(x)\n",
    "    shape_g = K.int_shape(gating)\n",
    "\n",
    "# Getting the x signal to the same shape as the gating signal\n",
    "    theta_x = layers.Conv2D(inter_shape, (2, 2), strides=(2, 2), padding='same')(x)  # 16\n",
    "    shape_theta_x = K.int_shape(theta_x)\n",
    "\n",
    "# Getting the gating signal to the same number of filters as the inter_shape\n",
    "    phi_g = layers.Conv2D(inter_shape, (1, 1), padding='same')(gating)\n",
    "    upsample_g = layers.Conv2DTranspose(inter_shape, (3, 3),\n",
    "                                 strides=(shape_theta_x[1] // shape_g[1], shape_theta_x[2] // shape_g[2]),\n",
    "                                 padding='same')(phi_g)  # 16\n",
    "\n",
    "    concat_xg = layers.add([upsample_g, theta_x])\n",
    "    act_xg = layers.Activation('relu')(concat_xg)\n",
    "    psi = layers.Conv2D(1, (1, 1), padding='same')(act_xg)\n",
    "    sigmoid_xg = layers.Activation('sigmoid')(psi)\n",
    "    shape_sigmoid = K.int_shape(sigmoid_xg)\n",
    "    upsample_psi = layers.UpSampling2D(size=(shape_x[1] // shape_sigmoid[1], shape_x[2] // shape_sigmoid[2]))(sigmoid_xg)  # 32\n",
    "\n",
    "    upsample_psi = repeat_elem(upsample_psi, shape_x[3])\n",
    "\n",
    "    y = layers.multiply([upsample_psi, x])\n",
    "\n",
    "    result = layers.Conv2D(shape_x[3], (1, 1), padding='same')(y)\n",
    "    result_bn = layers.BatchNormalization()(result)\n",
    "    return result_bn\n",
    "\n",
    "def Attention_ResUNet(input_shape, NUM_CLASSES=1, dropout_rate=0.0, batch_norm=True):\n",
    "    '''\n",
    "    Residual UNet, with attention \n",
    "    \n",
    "    '''\n",
    "    # network structure\n",
    "    FILTER_NUM = 64 # number of basic filters for the first layer\n",
    "    FILTER_SIZE = 3 # size of the convolutional filter\n",
    "    UP_SAMP_SIZE = 2 # size of upsampling filters\n",
    "    # input data\n",
    "    # dimension of the image depth\n",
    "    inputs = layers.Input(input_shape, dtype=tf.float32)\n",
    "    axis = 3\n",
    "\n",
    "    # Downsampling layers\n",
    "    # DownRes 1, double residual convolution + pooling\n",
    "    conv_128 = res_conv_block(inputs, FILTER_SIZE, FILTER_NUM, dropout_rate, batch_norm)\n",
    "    pool_64 = layers.MaxPooling2D(pool_size=(2,2))(conv_128)\n",
    "    # DownRes 2\n",
    "    conv_64 = res_conv_block(pool_64, FILTER_SIZE, 2*FILTER_NUM, dropout_rate, batch_norm)\n",
    "    pool_32 = layers.MaxPooling2D(pool_size=(2,2))(conv_64)\n",
    "    # DownRes 3\n",
    "    conv_32 = res_conv_block(pool_32, FILTER_SIZE, 4*FILTER_NUM, dropout_rate, batch_norm)\n",
    "    pool_16 = layers.MaxPooling2D(pool_size=(2,2))(conv_32)\n",
    "    # DownRes 4\n",
    "    conv_16 = res_conv_block(pool_16, FILTER_SIZE, 8*FILTER_NUM, dropout_rate, batch_norm)\n",
    "    pool_8 = layers.MaxPooling2D(pool_size=(2,2))(conv_16)\n",
    "    # DownRes 5, convolution only\n",
    "    conv_8 = res_conv_block(pool_8, FILTER_SIZE, 16*FILTER_NUM, dropout_rate, batch_norm)\n",
    "\n",
    "    # Upsampling layers\n",
    "    # UpRes 6, attention gated concatenation + upsampling + double residual convolution\n",
    "    gating_16 = gating_signal(conv_8, 8*FILTER_NUM, batch_norm)\n",
    "    att_16 = attention_block(conv_16, gating_16, 8*FILTER_NUM)\n",
    "    up_16 = layers.UpSampling2D(size=(UP_SAMP_SIZE, UP_SAMP_SIZE), data_format=\"channels_last\")(conv_8)\n",
    "    up_16 = layers.concatenate([up_16, att_16], axis=axis)\n",
    "    up_conv_16 = res_conv_block(up_16, FILTER_SIZE, 8*FILTER_NUM, dropout_rate, batch_norm)\n",
    "    # UpRes 7\n",
    "    gating_32 = gating_signal(up_conv_16, 4*FILTER_NUM, batch_norm)\n",
    "    att_32 = attention_block(conv_32, gating_32, 4*FILTER_NUM)\n",
    "    up_32 = layers.UpSampling2D(size=(UP_SAMP_SIZE, UP_SAMP_SIZE), data_format=\"channels_last\")(up_conv_16)\n",
    "    up_32 = layers.concatenate([up_32, att_32], axis=axis)\n",
    "    up_conv_32 = res_conv_block(up_32, FILTER_SIZE, 4*FILTER_NUM, dropout_rate, batch_norm)\n",
    "    # UpRes 8\n",
    "    gating_64 = gating_signal(up_conv_32, 2*FILTER_NUM, batch_norm)\n",
    "    att_64 = attention_block(conv_64, gating_64, 2*FILTER_NUM)\n",
    "    up_64 = layers.UpSampling2D(size=(UP_SAMP_SIZE, UP_SAMP_SIZE), data_format=\"channels_last\")(up_conv_32)\n",
    "    up_64 = layers.concatenate([up_64, att_64], axis=axis)\n",
    "    up_conv_64 = res_conv_block(up_64, FILTER_SIZE, 2*FILTER_NUM, dropout_rate, batch_norm)\n",
    "    # UpRes 9\n",
    "    gating_128 = gating_signal(up_conv_64, FILTER_NUM, batch_norm)\n",
    "    att_128 = attention_block(conv_128, gating_128, FILTER_NUM)\n",
    "    up_128 = layers.UpSampling2D(size=(UP_SAMP_SIZE, UP_SAMP_SIZE), data_format=\"channels_last\")(up_conv_64)\n",
    "    up_128 = layers.concatenate([up_128, att_128], axis=axis)\n",
    "    up_conv_128 = res_conv_block(up_128, FILTER_SIZE, FILTER_NUM, dropout_rate, batch_norm)\n",
    "\n",
    "    # 1*1 convolutional layers\n",
    "    \n",
    "    conv_final = layers.Conv2D(NUM_CLASSES, kernel_size=(1,1))(up_conv_128)\n",
    "    conv_final = layers.BatchNormalization(axis=axis)(conv_final)\n",
    "    conv_final = layers.Activation('softmax')(conv_final)\n",
    "\n",
    "    # Model integration\n",
    "    model = models.Model(inputs, conv_final, name=\"AttentionResUNet\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load images and sort masks and images\n",
    "train_img_dir = '../data/data_train/train/images/train/'\n",
    "train_mask_dir = '../data/data_train/train/masks/train/'\n",
    "\n",
    "img_list = os.listdir(train_img_dir)\n",
    "img_list.sort()\n",
    "msk_list = os.listdir(train_mask_dir)\n",
    "msk_list.sort()\n",
    "num_images = len(os.listdir(train_img_dir))\n",
    "print(\"Total number of training images are: \", num_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a function to perform additional preprocessing after datagen.\n",
    "#For example, scale images, convert masks to categorical, etc. \n",
    "def preprocess_data(img, mask, num_class):\n",
    "    #Scale images\n",
    "    img = img / 255. #This can be done in ImageDataGenerator but showing it outside as an example\n",
    "    #Convert mask to one-hot\n",
    "    labelencoder = LabelEncoder()\n",
    "    n, h, w, c = mask.shape  \n",
    "    mask = mask.reshape(-1,1)\n",
    "    mask = labelencoder.fit_transform(mask)\n",
    "    mask = mask.reshape(n, h, w, c)\n",
    "    mask = to_categorical(mask, num_class)\n",
    "      \n",
    "    return (img, mask)\n",
    "\n",
    "#Define the generator.\n",
    "#We are not doing any rotation or zoom to make sure mask values are not interpolated.\n",
    "#It is important to keep pixel values in mask as 0, 1, 2, 3, .....\n",
    "def trainGenerator(train_img_path, train_mask_path, num_class):\n",
    "    \n",
    "    img_data_gen_args = dict(horizontal_flip=True,\n",
    "                      vertical_flip=True,\n",
    "                      fill_mode='reflect')\n",
    "    \n",
    "    image_datagen = ImageDataGenerator(**img_data_gen_args)\n",
    "    mask_datagen = ImageDataGenerator(**img_data_gen_args)\n",
    "    \n",
    "    image_generator = image_datagen.flow_from_directory(\n",
    "        train_img_path,\n",
    "        class_mode = None,\n",
    "        color_mode = 'grayscale',\n",
    "        target_size=(512,512),\n",
    "        batch_size = batch_size,\n",
    "        seed = seed)\n",
    "    \n",
    "    mask_generator = mask_datagen.flow_from_directory(\n",
    "        train_mask_path,\n",
    "        class_mode = None,\n",
    "        color_mode = 'grayscale',\n",
    "        target_size=(512,512),\n",
    "        batch_size = batch_size,\n",
    "        seed = seed)\n",
    "    \n",
    "    train_generator = zip(image_generator, mask_generator)\n",
    "    \n",
    "    for (img, mask) in train_generator:\n",
    "        img, mask = preprocess_data(img, mask, num_class)\n",
    "        yield (img, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42 # Seed for trainGenerator\n",
    "batch_size = 1\n",
    "n_classes = 4\n",
    "LR = 0.005 #default value: 0.001\n",
    "optim = tf.keras.optimizers.Adam(LR)\n",
    "epochs = 5\n",
    "\n",
    "# Segmentation models losses can be combined together by '+' and scaled by integer or float factor\n",
    "# metrics = [tf.keras.metrics.MeanIoU(num_classes=n_classes)]\n",
    "dice_loss = sm.losses.DiceLoss() \n",
    "focal_loss = sm.losses.CategoricalFocalLoss()\n",
    "total_loss = dice_loss + (1 * focal_loss)\n",
    "metrics = [sm.metrics.IOUScore(threshold=None), sm.metrics.FScore(threshold=0.5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_img_path = '../data/data_train/train/images/'\n",
    "train_mask_path = '../data/data_train/train/masks/'\n",
    "train_img_gen = trainGenerator(train_img_path, train_mask_path, num_class=4)\n",
    "\n",
    "val_img_path = '../data/data_train/val/images/'\n",
    "val_mask_path = '../data/data_train/val/masks/'\n",
    "val_img_gen = trainGenerator(val_img_path, val_mask_path, num_class=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = train_img_gen.__next__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = val_img_gen.__next__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x.shape)\n",
    "print(y.shape)\n",
    "print(\"max value in image dataset is: \", x.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the model metrics and load model. \n",
    "num_train_images = len(os.listdir('../data/data_train/train/images/train/'))\n",
    "num_val_images = len(os.listdir('../data/data_train/val/images/val/'))\n",
    "steps_per_epoch = num_train_images//batch_size\n",
    "val_steps_per_epoch = num_val_images//batch_size\n",
    "\n",
    "IMG_HEIGHT = x.shape[1]\n",
    "IMG_WIDTH  = x.shape[2]\n",
    "IMG_CHANNELS = x.shape[3]\n",
    "input_shape = (IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS)\n",
    "\n",
    "print(f'Input shape: {input_shape}\\nTraining images: {num_train_images}\\nValidation images: {num_val_images}\\nSteps per epoch: {steps_per_epoch}\\nValidation steps per epoch: {val_steps_per_epoch}' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = ModelCheckpoint(\n",
    "    filepath = '../models/checkpoints/',\n",
    "    monitor='val_iou_score',\n",
    "    verbose=1,\n",
    "    save_best_only=True,\n",
    "    save_weights_only=False,\n",
    "    mode='max',\n",
    "    save_freq='epoch',\n",
    "    options=None,\n",
    "    initial_value_threshold=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latest_model = tf.train.latest_checkpoint(os.path.dirname('../models/checkpoints/saved_model.pb'))\n",
    "print(latest_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention Residual Unet\n",
    "att_res_unet_model = Attention_ResUNet(input_shape)\n",
    "att_res_unet_model.compile(optimizer=optim, loss=total_loss, metrics=['accuracy', metrics])\n",
    "#print(att_res_unet_model.summary())\n",
    "\n",
    "\n",
    "start = datetime.now() \n",
    "att_res_unet_history = att_res_unet_model.fit(train_img_gen,\n",
    "                    steps_per_epoch=steps_per_epoch,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=val_img_gen,\n",
    "                    validation_steps=val_steps_per_epoch,\n",
    "                    callbacks=[model_checkpoint])\n",
    "stop = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Execution time of the model \n",
    "execution_time_AttResUnet = stop-start\n",
    "print(\"Attention ResUnet execution time is: \", execution_time_AttResUnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type='attresunet'\n",
    "model_name = f'{model_type}_{loss_name}_{epochs}_{batch_size}_{LR}'\n",
    "att_res_unet_model.save(f'../models/{model_name}.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the history.history dict to a pandas DataFrame and save as csv for\n",
    "# future plotting\n",
    "att_res_unet_history_df = pd.DataFrame(att_res_unet_history.history) \n",
    "\n",
    "with open('custom_code_att_res_unet_history_df.csv', mode='w') as f:\n",
    "    att_res_unet_history_df.to_csv(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check history plots, one model at a time\n",
    "history = att_res_unet_history\n",
    "\n",
    "#plot the training and validation accuracy and loss at each epoch\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1, len(loss) + 1)\n",
    "plt.plot(epochs, loss, 'y', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "acc = history.history['jacard_coef']\n",
    "#acc = history.history['accuracy']\n",
    "val_acc = history.history['val_jacard_coef']\n",
    "#val_acc = history.history['val_accuracy']\n",
    "\n",
    "plt.plot(epochs, acc, 'y', label='Training Jacard')\n",
    "plt.plot(epochs, val_acc, 'r', label='Validation Jacard')\n",
    "plt.title('Training and validation Jacard')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Jacard')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = att_res_unet_model\n",
    "model_path = \"m../models/AttResUnet_10epochs.hdf5\"\n",
    "\n",
    "#Load one model at a time for testing.\n",
    "model = tf.keras.models.load_model(model_path, compile=False)\n",
    "\n",
    "\n",
    "test_img_number = random.randint(0, X_test.shape[0]-1)\n",
    "test_img = X_test[test_img_number]\n",
    "ground_truth=y_test[test_img_number]\n",
    "\n",
    "test_img_input=np.expand_dims(test_img, 0)\n",
    "prediction = (model.predict(test_img_input)[0,:,:,0] > 0.5).astype(np.uint8)\n",
    "\n",
    "plt.figure(figsize=(16, 8))\n",
    "plt.subplot(231)\n",
    "plt.title('Testing Image')\n",
    "plt.imshow(test_img, cmap='gray')\n",
    "plt.subplot(232)\n",
    "plt.title('Testing Label')\n",
    "plt.imshow(ground_truth[:,:,0], cmap='gray')\n",
    "plt.subplot(233)\n",
    "plt.title('Prediction on test image')\n",
    "plt.imshow(prediction, cmap='gray')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IoU for a single image\n",
    "from tensorflow.keras.metrics import MeanIoU\n",
    "n_classes = 2\n",
    "IOU_keras = MeanIoU(num_classes=n_classes)  \n",
    "IOU_keras.update_state(ground_truth[:,:,0], prediction)\n",
    "print(\"Mean IoU =\", IOU_keras.result().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate IoU for all test images and average\n",
    "IoU_values = []\n",
    "for img in range(0, X_test.shape[0]):\n",
    "    temp_img = X_test[img]\n",
    "    ground_truth=y_test[img]\n",
    "    temp_img_input=np.expand_dims(temp_img, 0)\n",
    "    prediction = (model.predict(temp_img_input)[0,:,:,0] > 0.5).astype(np.uint8)\n",
    "    \n",
    "    IoU = MeanIoU(num_classes=n_classes)\n",
    "    IoU.update_state(ground_truth[:,:,0], prediction)\n",
    "    IoU = IoU.result().numpy()\n",
    "    IoU_values.append(IoU)\n",
    "\n",
    "    print(IoU)\n",
    "    \n",
    "\n",
    "\n",
    "df = pd.DataFrame(IoU_values, columns=[\"IoU\"])\n",
    "df = df[df.IoU != 1.0]    \n",
    "mean_IoU = df.mean().values\n",
    "print(\"Mean IoU is: \", mean_IoU)    "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "27bfc2e7f7c4c223eebafb71203e56e1caf76124c3948973e06b9c38250454df"
  },
  "kernelspec": {
   "display_name": "Python 3.9.8 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
