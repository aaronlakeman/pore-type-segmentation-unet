{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import cv2\n",
    "import pickle\n",
    "\n",
    "from keras.models import Model\n",
    "import os\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a function to perform additional preprocessing after datagen.\n",
    "#For example, scale images, convert masks to categorical, etc. \n",
    "def preprocess_data(img, mask, num_class):\n",
    "    #Scale images\n",
    "    img = img / 255. #This can be done in ImageDataGenerator but showing it outside as an example\n",
    "    #Convert mask to one-hot\n",
    "    labelencoder = LabelEncoder()\n",
    "    n, h, w = mask.shape  \n",
    "    mask = mask.reshape(-1,1)\n",
    "    mask = labelencoder.fit_transform(mask)\n",
    "    mask = mask.reshape(n, h, w)\n",
    "    mask = to_categorical(mask, num_class)\n",
    "      \n",
    "    return img, mask\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Capture training image info as a list\n",
    "train_images = []\n",
    "\n",
    "for directory_path in glob.glob(\"../data/small_batch/train_images/\"):\n",
    "    for img_path in glob.glob(os.path.join(directory_path, \"*.tif\")):\n",
    "        img = cv2.imread(img_path, cv2.IMREAD_COLOR)       \n",
    "        img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "        train_images.append(img)\n",
    "#Convert list to array for machine learning processing        \n",
    "train_images = np.array(train_images)\n",
    "\n",
    "#Capture mask/label info as a list\n",
    "train_masks = [] \n",
    "for directory_path in glob.glob(\"../data/small_batch/train_masks/\"):\n",
    "    for mask_path in glob.glob(os.path.join(directory_path, \"*.tif\")):\n",
    "        mask = cv2.imread(mask_path, 0)\n",
    "        train_masks.append(mask)\n",
    "#Convert list to array for machine learning processing          \n",
    "train_masks = np.array(train_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_images\n",
    "y_train = train_masks\n",
    "y_train = np.expand_dims(y_train, axis=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(y_train)\n",
    "labelencoder = LabelEncoder()\n",
    "n, h, w, c = y_train.shape  \n",
    "y_train = y_train.reshape(-1,1)\n",
    "y_train = labelencoder.fit_transform(y_train)\n",
    "y_train = y_train.reshape(n, h, w, c)\n",
    "mask = to_categorical(y_train, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load VGG16 model wothout classifier/fully connected layers\n",
    "#Load imagenet weights that we are going to use as feature generators\n",
    "VGG_model = VGG16(weights='imagenet', include_top=False, input_shape=(512, 512, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make loaded layers as non-trainable. This is important as we want to work with pre-trained weights\n",
    "for layer in VGG_model.layers:\n",
    "\tlayer.trainable = False\n",
    "    \n",
    "VGG_model.summary()  #Trainable parameters will be 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#After the first 2 convolutional layers the image dimension changes. \n",
    "#So for easy comparison to Y (labels) let us only take first 2 conv layers\n",
    "#and create a new model to extract features\n",
    "#New model with only first 2 conv layers\n",
    "new_model = Model(inputs=VGG_model.input, outputs=VGG_model.get_layer('block1_conv2').output)\n",
    "new_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now, let us apply feature extractor to our training data\n",
    "features=new_model.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot features to view them\n",
    "square = 4\n",
    "ix=1\n",
    "\n",
    "fig = plt.figure(figsize=(12,12))\n",
    "for _ in range(square):\n",
    "    for _ in range(square):\n",
    "        fig.add_subplot(square, square, ix)\n",
    "        plt.imshow(features[0,:,:,ix-1], cmap='gray')\n",
    "        plt.axis('off')\n",
    "        ix +=1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reassign 'features' as X to make it easy to follow\n",
    "X=features\n",
    "X = X.reshape(-1, X.shape[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reshape Y to match X\n",
    "Y = y_train.reshape(-1)\n",
    "print(Y.shape)\n",
    "#Combine X and Y into a dataframe to make it easy to drop all rows with Y values 0\n",
    "#In our labels Y values 0 = unlabeled pixels. \n",
    "dataset = pd.DataFrame(X)\n",
    "dataset['Label'] = Y\n",
    "print(dataset['Label'].unique())\n",
    "print(dataset['Label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##If we do not want to include pixels with value 0 \n",
    "##e.g. Sometimes unlabeled pixels may be given a value 0.\n",
    "dataset = dataset[dataset['Label'] != 0]\n",
    "\n",
    "#Redefine X and Y for Random Forest\n",
    "X_for_training = dataset.drop(labels = ['Label'], axis=1)\n",
    "X_for_training = X_for_training.values  #Convert to array\n",
    "Y_for_training = dataset['Label']\n",
    "Y_for_training = Y_for_training.values  #Convert to array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_for_training.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#XGBOOST\n",
    "import xgboost as xgb\n",
    "model = xgb.XGBClassifier()\n",
    "\n",
    "# Train the model on training data\n",
    "model.fit(X_for_training, Y_for_training) \n",
    "\n",
    "#Save model for future use\n",
    "filename = 'model_XG.sav'\n",
    "pickle.dump(model, open(filename, 'wb'))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.8 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "fce1c7022bfe12e6241fab6e134680d8fdeb514a4f69a8524f2634ae3ff58bfb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
